{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAg5OvwnBSci"
      },
      "outputs": [],
      "source": [
        "# !sudo apt-get -y install fuse3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkuujLRICLa_"
      },
      "outputs": [],
      "source": [
        "# !sudo ln -s /bin/fusermount /bin/fusermount3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZTrJPm1ItbQ"
      },
      "outputs": [],
      "source": [
        "# !wget https://downloads.rclone.org/v1.64.2/rclone-v1.64.2-linux-amd64.deb\n",
        "# !apt install ./rclone-v1.64.2-linux-amd64.deb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McalAM8GxgOi"
      },
      "outputs": [],
      "source": [
        "# #@markdown Run this cell, and in rclone config, select `n` for new remote, name it `onedrive` and check the number in the list corresponding to onedrive (`26` in the current version). Then press enter for `client_id` and for `client_secret`, then `n` to avoid avanced config and `n` for auto config.\n",
        "# #@markdown Then paste the access token code generated in your machine, and select number 1 for \"Onedrive personal\" or \"Onedrive business\"\n",
        "# #@markdown Then, in \"found drives\" check that the configuration is ok, depending on the one you want to mount. Then select `y` twice and `q` to quit the configuration.\n",
        "# !rclone config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTOari7r6J1x"
      },
      "outputs": [],
      "source": [
        "# !sudo mkdir /content/onedrive\n",
        "# !nohup rclone --vfs-cache-mode writes mount onedrive: /content/onedrive &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ox0b1sPhbzj"
      },
      "outputs": [],
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXRBOvP40geO"
      },
      "outputs": [],
      "source": [
        "!pip install mutagen\n",
        "!pip install librosa\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install https://github.com/kpu/kenlm/archive/master.zip\n",
        "!pip install pyctcdecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EajraRWnoLV"
      },
      "outputs": [],
      "source": [
        "import torchaudio\n",
        "import librosa\n",
        "import numpy as np\n",
        "from mutagen.mp3 import MP3\n",
        "import requests\n",
        "from typing import Tuple\n",
        "\n",
        "class LocalLoader():\n",
        "    def __init__(self, sampling_rate=16000, duration=120):\n",
        "        self.sampling_rate = sampling_rate\n",
        "\n",
        "        # decide duration for each time read an audio\n",
        "        self.duration = duration\n",
        "\n",
        "    def load(self, file_path: str, segment: int):\n",
        "        '''\n",
        "        Load audio with a specific segment.\n",
        "        '''\n",
        "        audio = None\n",
        "        file_sampling_rate = None\n",
        "        startingTime = self.duration*(segment - 1)\n",
        "\n",
        "        if MP3(file_path).info.length >= startingTime:\n",
        "            # if segment satisfied, read the segment\n",
        "            audio, file_sampling_rate = librosa.load(file_path, sr=self.sampling_rate, offset = startingTime, duration=self.duration)\n",
        "        else:\n",
        "            # raise out of segment\n",
        "            raise Exception('Out of segment at' + str(file_path))\n",
        "\n",
        "        return audio, file_sampling_rate\n",
        "\n",
        "\n",
        "class LocalSplitter(LocalLoader):\n",
        "    def __init__(self, sampling_rate=16000, chunk_size=5, duration=120):\n",
        "        assert duration % chunk_size == 0\n",
        "\n",
        "        super().__init__(sampling_rate, duration)\n",
        "        self.chunk_size = chunk_size\n",
        "\n",
        "    def load(self, file_path: str, segment: int):\n",
        "        '''\n",
        "        Load and split the audio to some chunks (with a given size).\n",
        "        The audio file is loaded with a specific segment and duration.\n",
        "        '''\n",
        "        #load audio\n",
        "        fullAudio, file_sampling_rate = super().load(file_path,segment)\n",
        "        #number of floats need to use to store a chunk of audio\n",
        "        number_per_chunk = self.chunk_size * self.sampling_rate\n",
        "        # the size of final chunk (the final chunk may be shorter than the others)\n",
        "        finalChunk_size = fullAudio.shape[0] % number_per_chunk\n",
        "\n",
        "        # read chunks from audio except for the final chunk\n",
        "        audio_chunks = None\n",
        "        if int(fullAudio.shape[0] / number_per_chunk) > 0:\n",
        "            audio_chunks = fullAudio[0 : fullAudio.shape[0] - finalChunk_size].reshape(int(fullAudio.shape[0] / number_per_chunk), -1)\n",
        "\n",
        "        # final chunk in the audio\n",
        "        final_chunk = None\n",
        "        # read the final chunk\n",
        "        if finalChunk_size > 0:\n",
        "            final_chunk = np.zeros(number_per_chunk)\n",
        "            final_chunk[0 : finalChunk_size] = fullAudio[fullAudio.shape[0] - finalChunk_size : fullAudio.shape[0]]\n",
        "\n",
        "        return audio_chunks, final_chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZNwZFm_n0U6"
      },
      "outputs": [],
      "source": [
        "from mutagen.mp3 import MP3\n",
        "import pathlib\n",
        "import json\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "class FileTranscripter(LocalSplitter):\n",
        "    def __init__(self, model_inferencer, processor_feature_extractor, processor_tokenizer_decoder, device, sampling_rate=16000, chunk_size=5,duration=120):\n",
        "        super().__init__(sampling_rate, chunk_size, duration)\n",
        "        # device\n",
        "        self.device = device\n",
        "        # model to use\n",
        "        self.model_inferencer = model_inferencer\n",
        "        self.processor_tokenizer_decoder = processor_tokenizer_decoder\n",
        "        self.processor_feature_extractor = processor_feature_extractor\n",
        "\n",
        "\n",
        "    def transcript_segment(self, file_path: str, segment: int):\n",
        "        '''\n",
        "        Transcript a a segment of audio (the first segment is 1)\n",
        "        '''\n",
        "        try:\n",
        "            audio_chunks, final_audio_chunk = self.load(file_path= file_path, segment= segment)\n",
        "        except Exception as exc:\n",
        "            print(f'{file_path}, segment {segment} ', end='')\n",
        "            print(str(exc))\n",
        "            return None\n",
        "\n",
        "        result = []\n",
        "\n",
        "        # transcript splitted chunks in the segment\n",
        "        if audio_chunks is not None:\n",
        "            input_chunks = self.processor_feature_extractor(audio_chunks)\n",
        "            input_chunks.to(self.device)\n",
        "            output_chunks = self.model_inferencer(input_chunks)\n",
        "\n",
        "            result = self.processor_tokenizer_decoder(output_chunks)\n",
        "\n",
        "        # transcript the final chunk\n",
        "        if final_audio_chunk is not None:\n",
        "            input_final_chunk = self.processor_feature_extractor(final_audio_chunk)\n",
        "            input_final_chunk.to(self.device)\n",
        "            output_final_chunk = self.model_inferencer(input_final_chunk)\n",
        "\n",
        "            result.append(self.processor_tokenizer_decoder(output_final_chunk)[0])\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "    def transcript_file(self, file_path: str, starting_segment: int):\n",
        "        '''\n",
        "        Transcript the audio strat from a specific segment by transcripting the segments one by one\n",
        "        '''\n",
        "        # duration of the audio file\n",
        "        file_duration = MP3(file_path).info.length\n",
        "        # number of segment that the audio file has\n",
        "        num_of_segment = int(file_duration / self.duration) + int((file_duration - self.duration * int(file_duration / self.duration)) > 0)\n",
        "\n",
        "        result = [0] * num_of_segment\n",
        "\n",
        "        # transcript segment one by one\n",
        "        for segment in range(starting_segment, num_of_segment + 1, 1):\n",
        "            result[segment - 1] = self.transcript_segment(file_path, segment)\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "    def transcript_write_file(self, file_path: str, des_path: str, starting_segment: int, reading_state_path: str, stop_path :str):\n",
        "        '''\n",
        "        Transcript the audio strat from a specific segment by transcripting the segments one by one and write to a text file.\n",
        "        Also, save the reading state to a json file.\n",
        "        '''\n",
        "        # duration of the audio file\n",
        "        file_duration = MP3(file_path).info.length\n",
        "        # number of segment that the audio file has\n",
        "        num_of_segment = int(file_duration / self.duration) + int((file_duration - self.duration * int(file_duration / self.duration)) > 0)\n",
        "\n",
        "        # load the reading state\n",
        "        with open(reading_state_path, 'r') as state_read:\n",
        "            reading_state = json.load(state_read)\n",
        "\n",
        "        for segment in range(starting_segment, num_of_segment + 1, 1):\n",
        "            # check if require to terminate\n",
        "            with open(stop_path, 'r') as f:\n",
        "                stop = int(f.read())\n",
        "\n",
        "            if stop != 0:\n",
        "                print('Termination required, check the stop')\n",
        "                # update segment in reading state\n",
        "                with open(reading_state_path, 'w') as state_write:\n",
        "                    reading_state[\"final_read_segment\"] = segment - 1\n",
        "                    json.dump(reading_state, state_write)\n",
        "                state_read.close()\n",
        "                sys.exit()\n",
        "\n",
        "            # transcript the segment\n",
        "            segment_transcript = self.transcript_segment(file_path, segment)\n",
        "\n",
        "            transcript  = ''\n",
        "            for chunk_transcript in segment_transcript:\n",
        "                transcript += (chunk_transcript + '\\n')\n",
        "\n",
        "            # write to the text file\n",
        "            with open(des_path, 'a', encoding='utf8') as transcript_write:\n",
        "                transcript_write.write(transcript)\n",
        "\n",
        "            with open(reading_state_path, 'w') as state_write:\n",
        "                  reading_state[\"final_read_segment\"] = segment\n",
        "                  json.dump(reading_state, state_write)\n",
        "\n",
        "\n",
        "class FolderTranscripter(FileTranscripter):\n",
        "    def __init__(self, model_inferencer, processor_feature_extractor, processor_tokenizer_decoder, device, instruction_path, reading_state_name):\n",
        "        with open(instruction_path, 'r') as read_instruction:\n",
        "            instruction = json.load(read_instruction)\n",
        "\n",
        "        super().__init__(model_inferencer, processor_feature_extractor, processor_tokenizer_decoder, device, int(instruction[\"sampling_rate\"]), int(instruction[\"chunk_size\"]), int(instruction[\"duration\"]))\n",
        "\n",
        "        self.stop_path = instruction[\"stop_path\"]\n",
        "        self.data_folder_path = instruction[\"data_folder_path\"]\n",
        "\n",
        "        self.reading_state_path = pathlib.Path(self.data_folder_path) / reading_state_name\n",
        "        data_folder = pathlib.Path(self.data_folder_path)\n",
        "\n",
        "        #create a reading state file for a folder if not exists\n",
        "        if not self.reading_state_path.exists():\n",
        "            data_folder.touch(reading_state_name)\n",
        "            print(f'Created reading_state for {self.data_folder_path}')\n",
        "\n",
        "            initialize_reading_state = {\"last_file_reading\": \"\", \"final_read_segment\": 0}\n",
        "            with self.reading_state_path.open('w') as initialize_state_write:\n",
        "                json.dump(initialize_reading_state, initialize_state_write)\n",
        "        else:\n",
        "            print(f'{self.data_folder_path} has already had a reading state')\n",
        "\n",
        "\n",
        "    def transcript_write_folder(self, transcript_folder_name):\n",
        "        '''\n",
        "        Transcript all audio files in the folder by transcripting its files one by ones.\n",
        "        Also, save the reading state to a json file.\n",
        "        '''\n",
        "        data_folder = pathlib.Path(self.data_folder_path)\n",
        "        audio_folder = data_folder / 'audio'\n",
        "        transcript_folder = data_folder / transcript_folder_name\n",
        "\n",
        "        assert audio_folder.exists() == True\n",
        "\n",
        "        finished_files = []\n",
        "\n",
        "        # create a baseline-transcript folder to store transcripted text file if it not exists\n",
        "        if not transcript_folder.exists():\n",
        "            transcript_folder.mkdir(parents=False)\n",
        "            print('Create baseline-transcript folder')\n",
        "        else:\n",
        "        # if the transcript folder, save all the file names\n",
        "            finished_files = [str(file.stem) for file in transcript_folder.iterdir()]\n",
        "\n",
        "        # load the reading state and complete the unfinished audio from the final read segment + 1\n",
        "        with open(self.reading_state_path, 'r+') as state_read:\n",
        "            last_read_inf = json.load(state_read)\n",
        "            # if the final read file has not been completed\n",
        "            # transcript it starting (starting from the untranscripted segment)\n",
        "            if last_read_inf[\"last_file_reading\"] != \"\":\n",
        "                print(f'Unfinished file: {data_folder / last_read_inf[\"last_file_reading\"]}, segment: {last_read_inf[\"final_read_segment\"]}')\n",
        "\n",
        "                self.transcript_write_file(audio_folder / (last_read_inf[\"last_file_reading\"] + '.mp3'), transcript_folder / (last_read_inf[\"last_file_reading\"] + '.txt'), int(last_read_inf[\"final_read_segment\"]) + 1, self.reading_state_path, self.stop_path)\n",
        "                last_read_inf[\"last_file_reading\"] = \"\"\n",
        "                last_read_inf[\"final_read_segment\"] = 0\n",
        "\n",
        "                print(f'Completed unfinised file {data_folder / last_read_inf[\"last_file_reading\"]}')\n",
        "            else:\n",
        "                print('No unfinished file')\n",
        "\n",
        "        with open(self.reading_state_path, 'w') as state_write:\n",
        "            json.dump(last_read_inf, state_write)\n",
        "\n",
        "        # transcript audios in the folder\n",
        "        for audio_file_path in audio_folder.iterdir():\n",
        "            audio_file_name = str(audio_file_path.stem)\n",
        "\n",
        "            # skip the audio files that has already been completed\n",
        "            if  audio_file_name in finished_files:\n",
        "                print(f'Already complete {audio_folder / audio_file_name}\\n--------------------------------------')\n",
        "                continue\n",
        "\n",
        "            # create text file to store transcript (same name as its audio)\n",
        "            transcript_file_name = str(audio_file_path.stem) + '.txt'\n",
        "            transcript_folder.touch(transcript_file_name)\n",
        "\n",
        "            # write the name of the transcripting file to the reading state\n",
        "            with open(self.reading_state_path, 'w') as f:\n",
        "                last_read_inf[\"last_file_reading\"] = audio_file_name\n",
        "                last_read_inf[\"final_read_segment\"] = 0\n",
        "                json.dump(last_read_inf, f)\n",
        "\n",
        "            # transcript the file\n",
        "            print(f'Transcripting {str(transcript_folder / transcript_file_name)}')\n",
        "            self.transcript_write_file(str(audio_file_path), str(transcript_folder / transcript_file_name), 1, self.reading_state_path, self.stop_path)\n",
        "\n",
        "            # if completed, update the reading state file\n",
        "            with open(self.reading_state_path, 'w') as f:\n",
        "                last_read_inf[\"last_file_reading\"] = \"\"\n",
        "                last_read_inf[\"final_read_segment\"] = 0\n",
        "                json.dump(last_read_inf, f)\n",
        "\n",
        "            finished_files.append(audio_file_name)\n",
        "            print(f'Completed {data_folder / audio_file_name}\\n---------------------------------------')\n",
        "\n",
        "        print(f'COMPLETED ALL AUDIO IN FOLDER {data_folder}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iNRBFrUzpWv"
      },
      "outputs": [],
      "source": [
        "from transformers.utils.hub import cached_file\n",
        "from importlib.machinery import SourceFileLoader\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ProcessorWithLM\n",
        "from IPython.lib.display import Audio\n",
        "import json\n",
        "import time\n",
        "\n",
        "# model_name = \"nguyenvulebinh/wav2vec2-base-vi-vlsp2020\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# # load model without LM\n",
        "# model = SourceFileLoader(\"model\", cached_file(model_name,filename=\"model_handling.py\")).load_module().Wav2Vec2ForCTC.from_pretrained(model_name)\n",
        "# processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
        "\n",
        "# model.to(device)\n",
        "\n",
        "# model_inferencer = lambda input : model(**input)\n",
        "# processor_feature_extractor = lambda chunks : processor.feature_extractor(chunks, sampling_rate=16000, return_tensors='pt')\n",
        "# processor_tokenizer_decoder = lambda chunks :[processor.tokenizer.decode(chunks.logits[i].unsqueeze(0).argmax(dim=-1)[0].detach().cpu().numpy()) for i in range(chunks.logits.shape[0])]\n",
        "\n",
        "# load model with LM\n",
        "model_name = \"nguyenvulebinh/wav2vec2-large-vi-vlsp2020\"\n",
        "model = SourceFileLoader(\"model\", cached_file(model_name,filename=\"model_handling.py\")).load_module().Wav2Vec2ForCTC.from_pretrained(model_name)\n",
        "processor = Wav2Vec2ProcessorWithLM.from_pretrained(model_name)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "model_inferencer = lambda input : model(**input)\n",
        "processor_feature_extractor = lambda chunks : processor.feature_extractor(chunks, sampling_rate=16000, return_tensors='pt')\n",
        "processor_tokenizer_decoder = lambda chunks : [processor.decode(chunks.logits[i].unsqueeze(0).cpu().detach().numpy()[0], beam_width=100).text for i in range(chunks.logits.shape[0])]\n",
        "\n",
        "\n",
        "# transcript all files in a folder\n",
        "folder_transcript = FolderTranscripter(model_inferencer, processor_feature_extractor, processor_tokenizer_decoder, device, 'path to transcript_instruction.json', 'reading_state_wav.json')\n",
        "\n",
        "folder_transcript.transcript_write_folder('baseline_transcript_wav2vec2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XN6seLFdClO1"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "import torch\n",
        "# load model and processor\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large\")\n",
        "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"vietnamese\", task=\"transcribe\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "processor_feature_input = lambda x : processor(x, sampling_rate=16000, return_tensors=\"pt\").input_features\n",
        "\n",
        "model_inferencer = lambda input_features : model.generate(input_features)\n",
        "# decode token ids to text\n",
        "processor_decoder = lambda pred : processor.batch_decode(pred, skip_special_tokens=True)\n",
        "\n",
        "folder_transcript = FolderTranscripter(model_inferencer, processor_feature_input, processor_decoder, device, 'drive/MyDrive/testaudio/transcript_instruction.json', 'reading_state_whisper.json')\n",
        "folder_transcript.transcript_write_folder('baseline_transcript_whisper')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
